## Human-[Pepper](https://www.aldebaran.com/en/pepper/) Interaction with [MODIM](https://bitbucket.org/mtlazaro/modim/src/master/) interface

<div align="center">
  <video src="https://github.com/mms-ngl/hri/assets/150866036/5bf26c2b-3a9c-496c-a3c2-9a1a70fdbbbb" />
</div>

> To effectively assist shopping malls visitors, a social humanoid robot Pepper identifies the individuals it interacts with to offer personalized assistance. Personalized actions for visitor categories are as follows:\
> A. For individuals with deafness, the action exclusively displays images and texts without any spoken output.\
> B. For individuals with blindness, the action solely delivers spoken information.\
> C. If the visitor is elderly, the action presents larger texts and also provides spoken output.\
> D. For individuals without deafness, blindness, or being elderly, the action displays standard-sized text and delivers spoken information.

Project for "Human-Robot Interaction" course. 

Paper: “Personalized short-term multi-modal interaction for social robots assisting users in shopping malls” 

Authors: Luca Iocchi, Maria Teresa L´azaro, Laurent Jeanpierre, Abdel-Illah Mouaddib

### 📝 Project documentation
[**PAPER PRESENTATION**](https://github.com/mms-ngl/hri/blob/main/paper_presentation.pdf)

[**REPORT**](https://github.com/mms-ngl/hri/blob/main/report.pdf)


